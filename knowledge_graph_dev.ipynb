{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"bolt://localhost:7687\"\n",
    "username =\"neo4j\"\n",
    "password = \"ilya3939\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_academic_paper = f\"\"\"\n",
    "## Creating a Knowledge Graph from Academic Papers\n",
    "\n",
    "### Objective\n",
    "Design a sophisticated algorithm to read academic papers and extract structured information for constructing a knowledge graph. This graph aims to elucidate the relationships between various methods, tasks prioritized within the research, their comparative superiority, and potential replacements for methods.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Node Identification and Labeling\n",
    "- **Entities and Concepts**: Identify primary entities such as methods, tasks, and research findings as nodes. Label these nodes with elementary types for consistency.\n",
    "- **Method** nodes represent algorithms or techniques introduced or discussed.\n",
    "- **Task** nodes represent problems or objectives the methods aim to solve or achieve.\n",
    "- **Finding** nodes encapsulate specific outcomes, comparisons, or advancements highlighted in the paper.\n",
    "\n",
    "### Allowed Node Labels:\n",
    "- **Method**: Represents algorithms or techniques introduced or discussed within the paper.\n",
    "- **Task**: Represents problems or objectives that methods aim to solve or achieve.\n",
    "- **Finding**: Encapsulates specific outcomes, comparisons, or advancements highlighted.\n",
    "- **Person**: Represents authors or researchers mentioned in the paper.\n",
    "- **Institution**: Represents universities, research centers, or organizations affiliated with the authors.\n",
    "- **Concept**: Represents fundamental concepts or theories underlying methods or tasks.\n",
    "- **Dataset**: Represents datasets used or introduced in the paper for experimentation or evaluation.\n",
    "\n",
    "#### Relationships\n",
    "- **Defining Relationships**: Establish clear relationships between nodes to reflect the paper's content accurately.\n",
    "- Use relationships like **\"addressesTask\"**, **\"outperformsMethod\"**, and **\"canBeReplacedWith\"** to demonstrate the interaction between methods and tasks, comparative superiority, and methodological replacements.\n",
    "- Ensure relationships are directly extracted from the text, clearly indicating the nature of connections between entities.\n",
    "\n",
    "### Allowed Relationship Types:\n",
    "- **addressesTask**: Indicates that a method is applied to address a specific task.\n",
    "- **outperformsMethod**: Indicates that one method outperforms another in terms of effectiveness, efficiency, or other metrics.\n",
    "- **canBeReplacedWith**: Suggests that one method can be replaced with another, offering similar or improved outcomes.\n",
    "- **authoredBy**: Links a paper or finding to its authors.\n",
    "- **affiliatedWith**: Associates authors or researchers with their respective institutions.\n",
    "- **usesDataset**: Indicates that a method utilizes a specific dataset for training, testing, or validation.\n",
    "- **buildsUpon**: Signifies that a method or concept builds upon previous work, indicating progression or enhancement.\n",
    "- **comparesWith**: Indicates a direct comparison between two or more methods or tasks within the paper.\n",
    "\n",
    "\n",
    "#### Handling Numerical Data and Dates\n",
    "- **Attributes**: Incorporate numerical data and dates as attributes of nodes where relevant, such as the publication year of the method or performance metrics of findings.\n",
    "- Follow the key-value format with camelCase keys, avoiding quotes within property values.\n",
    "\n",
    "#### Coreference Resolution\n",
    "- **Entity Consistency**: Maintain consistent identifiers for entities mentioned multiple times, using the most complete and unambiguous identifier found in the paper.\n",
    "- This ensures clarity and coherence in the knowledge graph, aiding in the accurate representation of relationships and findings.\n",
    "\n",
    "#### Graph Construction Guidelines\n",
    "- **Simplicity and Clarity**: Aim for a knowledge graph that is easily navigable, making the research's key points, methodologies, and conclusions accessible to a broad audience.\n",
    "- **Data Source**: Utilize academic papers as the primary source of information, focusing on sections that discuss methods, results, and conclusions for direct extraction of relevant data.\n",
    "\n",
    "### Example\n",
    "- **Nodes**:\n",
    "- Method: \"Convolutional Neural Network (CNN)\"\n",
    "- Task: \"Image Classification\"\n",
    "- Finding: \"Improved accuracy over previous methods by 5%\"\n",
    "- **Relationships**:\n",
    "- \"CNN\" **addressesTask** \"Image Classification\"\n",
    "- \"CNN\" **outperformsMethod** \"Traditional Neural Networks\"\n",
    "- \"CNN\" **canBeReplacedWith** \"ResNet for complex image tasks\"\n",
    "\n",
    "This structured approach to creating a knowledge graph from academic papers facilitates the extraction of insightful information regarding methodological innovations, task relevance, and the comparative effectiveness of various approaches within the research community.\n",
    "\"\"\"\n",
    "\n",
    "wiki_prompt = \"\"\"\n",
    "# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          prompt_for_academic_paper\n",
    "          ),\n",
    "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.run(document.page_content)\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TeXの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"./data/MAML/senstive.tex\")\n",
    "raw_documents = loader.load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "\n",
    "# Only take the first the raw_documents\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 普通にPDF=>文字だけを入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def convert_pdf_to_text(pdf_path, txt_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    content = \"\"\n",
    "    for page in pages:\n",
    "        content += page.page_content\n",
    "    with open(txt_path, \"w\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "#convert_pdf_to_text(\"./DB/PDF/meta-ticket.pdf\", \"./DB/TXT/meta-ticket.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFをテキストに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "DB_PATH = \"miniF2F/\"\n",
    "PDF_PATH = \"PDF/\"\n",
    "TXT_PATH = \"TXT/\"\n",
    "\n",
    "pdf_directory = os.path.join(DB_PATH, PDF_PATH)\n",
    "txt_directory = os.path.join(DB_PATH, TXT_PATH)\n",
    "\n",
    "def convert_pdf_to_text(pdf_path, txt_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    content = \"\"\n",
    "    for page in pages:\n",
    "        content += page.page_content\n",
    "    with open(txt_path, \"w\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Find all PDF files in the directory\n",
    "pdf_files = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
    "\n",
    "# Convert each PDF file to text\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    file_name = os.path.basename(pdf_file)\n",
    "    txt_file = os.path.join(txt_directory, os.path.splitext(file_name)[0] + \".txt\")\n",
    "    convert_pdf_to_text(pdf_file, txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストをKGに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "def convert_txt_to_documents(txt_path):\n",
    "    loader = TextLoader(txt_path)\n",
    "    raw_documents = loader.load()\n",
    "    text_splitter = TokenTextSplitter(chunk_size=4096, chunk_overlap=48)\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [04:29<00:00, 38.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Find all TXT files in the directory\n",
    "txt_files = glob.glob(os.path.join(txt_directory, \"*.txt\"))\n",
    "\n",
    "\"\"\"\n",
    "for txt_file in tqdm(txt_files):\n",
    "    documents = convert_txt_to_documents(txt_file)\n",
    "    for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "        extract_and_store_graph(d)\n",
    "\"\"\"\n",
    "\n",
    "# One by one\n",
    "documents = convert_txt_to_documents(txt_files[0])\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    extract_and_store_graph(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (n {name: \"Gptf\"})-[:COMPARESWITH|OUTPERFORMSMETHOD|USESDATASET|AFFILIATEDWITH|ADDRESSESTASK|USESMETHOD|BUILDSUPON|HASFINDING|IMPACTEDBY|CONTRIBUTEDTO*]-(relatedNodes)\n",
      "RETURN n, relatedNodes\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know the answer.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0.5, model=\"gpt-4-0125-preview\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0.5, model=\"gpt-4-0125-preview\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")\n",
    "cypher_chain.run(\"What do you know about Llemma?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グラフを削除したい場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the graph\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
